{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import shap\n",
    "from ydata_profiling import ProfileReport\n",
    "import sweetviz as sv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "***Problem:***\n",
    "1. received recipe does not match the customer's **diet**\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "diet = pd.read_csv('diet.csv', sep=',')\n",
    "recipes = pd.read_csv('recipes.csv', sep=',')\n",
    "requests = pd.read_csv('requests.csv', sep=',')\n",
    "reviews = pd.read_csv('reviews.csv', sep=',')\n",
    "\n",
    "# sv.analyze(diet).show_notebook()\n",
    "# sv.analyze(recipes).show_notebook()\n",
    "# sv.analyze(requests).show_notebook()\n",
    "# sv.analyze(reviews).show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the diet data\n",
    "diet = diet.dropna()\n",
    "# convert diet to numeric\n",
    "diet[\"Diet\"] = diet[\"Diet\"].map({\"Vegan\": 0, \"Vegetarian\": 1, \"Omnivore\": 2}).astype(int)\n",
    "\n",
    "# def classify_age(age):\n",
    "#     if age < 45:\n",
    "#         return 1\n",
    "#     elif 45 <= age < 65:\n",
    "#         return 2\n",
    "#     else:\n",
    "#         return 3\n",
    "# diet[\"Age\"] = diet[\"Age\"].apply(classify_age)\n",
    "\n",
    "# drop the age column\n",
    "# diet.drop(columns=['Age'], inplace=True)\n",
    "# diet.info()\n",
    "# diet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diet_dict_Omnivore = {\n",
    "#     'meat': 'Omnivore',\n",
    "#     'fish': 'Omnivore',\n",
    "#     'beef': 'Omnivore',\n",
    "#     'pork': 'Omnivore',\n",
    "#     'chicken': 'Omnivore',\n",
    "#     'chickens': 'Omnivore',\n",
    "#     'duck': 'Omnivore',\n",
    "#     'turkey': 'Omnivore',\n",
    "#     'lamb': 'Omnivore',\n",
    "#     'goat': 'Omnivore',\n",
    "#     'rabbit': 'Omnivore',\n",
    "#     'goose': 'Omnivore',\n",
    "#     'mutton': 'Omnivore',\n",
    "#     'ham': 'Omnivore',\n",
    "#     'sausage': 'Omnivore',\n",
    "#     'bacon': 'Omnivore',\n",
    "#     'salmon' : 'Omnivore',\n",
    "# }\n",
    "\n",
    "# diet_dict_Vegetarian = {\n",
    "#     'vegetarian': 'Vegetarian',\n",
    "#     'egg': 'Vegetarian',\n",
    "#     'eggs': 'Vegetarian',\n",
    "#     'cheese': 'Vegetarian',\n",
    "#     'milk': 'Vegetarian',\n",
    "#     'yogurt': 'Vegetarian',\n",
    "#     'cream': 'Vegetarian',\n",
    "#     'butter': 'Vegetarian',\n",
    "#     'honey': 'Vegetarian',\n",
    "#     'cheese': 'Vegetarian',\n",
    "#     'whip' : 'Vegetarian',\n",
    "#     'marshmallow' : 'Vegetarian',\n",
    "#     'pudding' : 'Vegetarian',\n",
    "#     'mayonnaise' : 'Vegetarian',\n",
    "# }\n",
    "list_Omnivore = [\n",
    "    \"chicken\", \"beef\", \"gelatin\", \"bacon\", \"shrimp\", \"ham\", \"hoisin\", \"pork\", \"sausage\", \"prosciutto\", \"lamb\", \"fish\", \"turkey\", \"tuna\", \"oyster\", \"salmon\", \"fillet\", \"trout\", \"scallop\", \"mussel\", \"anchovy\", \"ladyfinger\", \"steak\", \"halibut\", \"crab\", \"squid\", \"sirloin\", \"pancetta\", \"rump\", \"anchovy\", \"anchovies\", \"red snapper\", \"clam\", \"duck\", \"kielbasa\", \"bouillon\", \"tasso\", \"trassi\", \"venison\", \"lobster\", \"pimento\", \"flounder\", \"rabbit\", \"chorizo\", \"haddock\", \"sole\", \"mahi\", \"mortadella\", \"caviar\", \"herring\", \"liver\", \"roast\", \"sea bass\", \"quail\", \"cod\", \"lox\", \"suet\", \"speck\", \"nam pla\", \"perch\", \"sardine\", \"smelt\", \"pheasant\", \"lardon\", \"aspic\", \"tripe\", \"veal\", \"bresaola\", \"borscht\", \"alligator\", \"suet\", \"nuoc nam\", \"partridge\", \"goose\", \"succotash\", \"eel\", \"jamon\", \"jamsn\", \"calf\", \"quahog\", \"grouper\", \"foie gras\", \"rabbit\", \"pike\", \"abalone\", \"cottage roll\", \"glace de viande\"\n",
    "]\n",
    "\n",
    "# egg milk cheese butter\n",
    "list_Vegetarian = [\n",
    "    \"butter\", \"cheddar\", \"cheese\", \"cream\", \"milk\", \"egg\", \"mayonnaise\", \"pudding\", \"cool whip\", \"creme\", \"worcestershire sauce\", \"miracle whip\", \"yogurt\", \"feta\", \"ghee\", \"panir\", \"gruyere\", \"yoghurt\", \"puff pastry\", \"queso\", \"custard\", \"phyllo\", \"parmigiano\", \"fontina\", \"muffin\", \"bisquick\", \"mascarpone\", \"mozzarella\", \"pastry\", \"shortening\", \"icing\", \"pignolis\", \"amchur\", \"bagel\", \"crepes\", \"ricotta\", \"cake\", \"amaretti\", \"parmesan\", \"quark\", \"ranch\", \"rusk\", \"dolcelatte\", \"shortbread\", \"rarebit\"\n",
    "]\n",
    "\n",
    "# def classify_diet(ingredients):\n",
    "#     for ingredient in diet_dict:\n",
    "#         if ingredient in ingredients:\n",
    "#             return diet_dict[ingredient]\n",
    "#     return 'Vegan'\n",
    "\n",
    "# def classify_diet(ingredients):\n",
    "#     ingredients = ingredients.lower() \n",
    "#     words = ingredients.split()\n",
    "#     for ingredient in diet_dict_Omnivore:\n",
    "#         if ingredient in words:\n",
    "#             return diet_dict_Omnivore[ingredient]\n",
    "#     for ingredient in diet_dict_Vegetarian:\n",
    "#         if ingredient in words:\n",
    "#             return diet_dict_Vegetarian[ingredient]\n",
    "#     return 'Vegan'\n",
    "\n",
    "# def classify_diet(ingredients):\n",
    "#     ingredients.lower()\n",
    "#     for ingredient in list_Omnivore:\n",
    "#         if ingredients.find(ingredient) != -1:\n",
    "#             return 'Omnivore'\n",
    "#     for ingredient in list_Vegetarian:\n",
    "#         if ingredients.find(ingredient) != -1:\n",
    "#             return 'Vegetarian'\n",
    "#     return 'Vegan'\n",
    "\n",
    "def classify_diet(ingredients):\n",
    "    for ingredient in [x.lower() for x in ingredients]:\n",
    "        for ingredient_ominivore in list_Omnivore:\n",
    "            if ingredient_ominivore in ingredient:\n",
    "                return 'Omnivore'\n",
    "    for ingredient in [x.lower() for x in ingredients]:\n",
    "        for ingredient_vegetarian in list_Vegetarian:\n",
    "            if ingredient_vegetarian in ingredient:\n",
    "                return 'Vegetarian'\n",
    "    return 'Vegan'\n",
    "\n",
    "# recipes[\"Diet\"] = recipes[\"RecipeIngredientParts\"].apply(classify_diet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecipeDietType\n",
      "1    33390\n",
      "2    26033\n",
      "0    16181\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# clean up recipes data\n",
    "\n",
    "# add a column for total time\n",
    "recipes[\"TotalTime\"] = recipes[\"CookTime\"] + recipes[\"PrepTime\"]\n",
    "recipes.drop(columns=\"CookTime\", inplace=True)\n",
    "recipes.drop(columns=\"PrepTime\", inplace=True)\n",
    "\n",
    "# convert recipe category to numeric\n",
    "recipes[\"RecipeCategory\"] = recipes[\"RecipeCategory\"].map({\"Other\" : 1, \"Lunch\" : 2, \"One dish meal\" : 3, \"Bread\" : 4 , \"Breakfast\" : 5, \"Beverages\" : 6, \"Soup\" : 7}).astype(int)\n",
    "\n",
    "# convert recipe ingredient to numeric\n",
    "recipes[\"RecipeIngredientParts\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x : re.findall(r\"\\\"\\\\\\\"([a-zA-Z ]+)\\\\\\\"\\\"\", x))\n",
    "# recipes[\"RecipeIngredientParts\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x : \" \".join(x))\n",
    "recipes[\"RecipeDietType\"] = recipes[\"RecipeIngredientParts\"].apply(classify_diet).map({\"Vegan\": 0, \"Vegetarian\": 1, \"Omnivore\": 2}).astype(int)\n",
    "\n",
    "print(recipes[\"RecipeDietType\"].value_counts())\n",
    "\n",
    "# drop columns: Name, RecipeCategory, RecipeIngredientQuantities, RecipeIngredientParts, SaturatedFatContent, CholesterolContent,RecipeServings, RecipeYield\n",
    "recipes = recipes.drop(columns=[\"Name\",\"RecipeIngredientQuantities\",\"RecipeIngredientParts\", \"SaturatedFatContent\",\"CholesterolContent\",\"RecipeServings\",\"RecipeYield\"])\n",
    "# recipes = recipes.drop(columns=[\"Name\", \"RecipeIngredientQuantities\",\"RecipeIngredientParts\", \"SaturatedFatContent\",\"CholesterolContent\",\"FiberContent\", \"RecipeServings\",\"RecipeYield\"])\n",
    "\n",
    "# corr_matrix = recipes.corr()\n",
    "# print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the requests data\n",
    "requests = requests.dropna()\n",
    "\n",
    "# convert negative time to 0.0\n",
    "# requests[\"Time\"] = requests[\"Time\"].map(lambda x : 0.0 if x < 0.0 else x)\n",
    "requests[\"Time\"] = requests[\"Time\"]\n",
    "# print(requests[\"Time\"].value_counts())\n",
    "\n",
    "# convert falgs to numeric\n",
    "requests[\"HighCalories\"] = requests[\"HighCalories\"].astype(int)\n",
    "requests[\"HighProtein\"] = requests[\"HighProtein\"].map({\"Indifferent\": -1, \"Yes\": 1}).astype(int)\n",
    "requests[\"LowFat\"] = requests[\"LowFat\"].astype(bool)\n",
    "\n",
    "# convert indiffernt to -1\n",
    "requests[\"LowSugar\"] = requests[\"LowSugar\"].map({\"Indifferent\": -1, \"0\": 0}).astype(int)\n",
    "requests[\"HighFiber\"] = requests[\"HighFiber\"].astype(bool)\n",
    "\n",
    "# drop columns: HighProtein\n",
    "# requests.drop(columns=[\"HighProtein\"], inplace=True)\n",
    "\n",
    "# corr_matrix = requests.drop(columns=[\"AuthorId\", \"RecipeId\"]).corr()\n",
    "# print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the reviews data\n",
    "reviews.drop(columns=\"Rating\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 97381 entries, 18 to 140194\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Like                 97381 non-null  int32  \n",
      " 1   Age                  97381 non-null  int64  \n",
      " 2   RecipeCategory       97381 non-null  int32  \n",
      " 3   SodiumContent        97381 non-null  float64\n",
      " 4   CarbohydrateContent  97381 non-null  float64\n",
      " 5   ProteinContent       97381 non-null  float64\n",
      " 6   HighProtein          97381 non-null  int32  \n",
      " 7   LowSugar             97381 non-null  int32  \n",
      " 8   TimeSatisfied        97381 non-null  bool   \n",
      " 9   DietSatisfied        97381 non-null  bool   \n",
      " 10  CaloriesSatisfied    97381 non-null  bool   \n",
      " 11  FatSatisfied         97381 non-null  bool   \n",
      " 12  FiberSatisfied       97381 non-null  bool   \n",
      "dtypes: bool(5), float64(3), int32(4), int64(1)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "# merge the reviews and diet data\n",
    "training_dataset = reviews.merge(diet, on=\"AuthorId\", how=\"inner\")\n",
    "# merge feedback with the recipes data\n",
    "training_dataset = training_dataset.merge(recipes, on=\"RecipeId\", how=\"inner\")\n",
    "# merge feedback with the requests data\n",
    "training_dataset = training_dataset.merge(requests, on=[\"AuthorId\", \"RecipeId\"], how=\"inner\")\n",
    "\n",
    "# clean up the feedback data\n",
    "training_dataset.drop(columns=\"AuthorId\", inplace=True)\n",
    "training_dataset.drop(columns=\"RecipeId\", inplace=True)\n",
    "\n",
    "# add new column with the boolean value if the costumer's  request time and recipe time are not too different\n",
    "training_dataset[\"TimeSatisfied\"] = (training_dataset[\"TotalTime\"] - training_dataset[\"Time\"]).map(lambda x : True if abs(x) <= 1.0 else False)\n",
    "training_dataset.drop(columns=\"TotalTime\", inplace=True)\n",
    "training_dataset.drop(columns=\"Time\", inplace=True)\n",
    "\n",
    "# add new column with the boolean value if the costumer's diet and recipe diet type are the same\n",
    "training_dataset[\"DietSatisfied\"] = training_dataset[\"Diet\"] == training_dataset[\"RecipeDietType\"]\n",
    "training_dataset.drop(columns=\"Diet\", inplace=True)\n",
    "training_dataset.drop(columns=\"RecipeDietType\", inplace=True)\n",
    "\n",
    "# add new column with the boolean value if the recipes is high calories and the costumer's request is high calories true, select 500 as dividing line\n",
    "training_dataset[\"CaloriesSatisfied\"] = (training_dataset[\"HighCalories\"] == 1) & (training_dataset[\"Calories\"] >= 800)\n",
    "training_dataset.drop(columns=\"HighCalories\", inplace=True)\n",
    "training_dataset.drop(columns=\"Calories\", inplace=True)\n",
    "\n",
    "# add new column with the boolean value if the recipes is low fat and the costumer's request is low fat true, select 10 as dividing line\n",
    "training_dataset[\"FatSatisfied\"] = (training_dataset[\"LowFat\"] == 1) & (training_dataset[\"FatContent\"] <= 10)\n",
    "training_dataset.drop(columns=\"LowFat\", inplace=True)\n",
    "training_dataset.drop(columns=\"FatContent\", inplace=True)\n",
    "\n",
    "# add new column with the boolean value if the recipes is low sugar sa\n",
    "# if the costumer's request of low sugar is indifferent, then is true\n",
    "# 5 is the dividing line for low sugar\n",
    "# training_dataset[\"SugarSatisfied\"] = (training_dataset[\"LowSugar\"] == -1) | (training_dataset[\"LowSugar\"] == 1 & (training_dataset[\"SugarContent\"] <= 0.5))\n",
    "# training_dataset.drop(columns=\"LowSugar\", inplace=True)\n",
    "training_dataset.drop(columns=\"SugarContent\", inplace=True)\n",
    "\n",
    "\n",
    "# add new column with the boolean value if the recipes is high fiber and the costumer's request is high fiber true, select 4 as dividing line\n",
    "training_dataset[\"FiberSatisfied\"] = (training_dataset[\"HighFiber\"] == 1) & (training_dataset[\"FiberContent\"] >= 3.5)\n",
    "training_dataset.drop(columns=\"HighFiber\", inplace=True)\n",
    "training_dataset.drop(columns=\"FiberContent\", inplace=True)\n",
    "\n",
    "\n",
    "# extract the submission_dataset with the testid\n",
    "submission_dataset = training_dataset.dropna(subset=['TestSetId']).copy()\n",
    "submission_dataset.sort_values(by=['TestSetId'], inplace=True)\n",
    "submission_dataset.reset_index(drop=True, inplace=True)\n",
    "submission_dataset.drop(columns=\"TestSetId\", inplace=True)\n",
    "submission_dataset.drop(columns=\"Like\", inplace=True)\n",
    "\n",
    "# print(submission_dataset.head())\n",
    "\n",
    "# clean up the feedback data\n",
    "training_dataset = training_dataset.dropna(subset=['Like'])\n",
    "training_dataset.drop(columns=\"TestSetId\", inplace=True)\n",
    "training_dataset[\"Like\"] = training_dataset[\"Like\"].astype(int)\n",
    "\n",
    "# print(training_dataset[\"CaloriesSatisfied\"].value_counts())\n",
    "# print(training_dataset[\"Like\"].value_counts())\n",
    "\n",
    "# corr_matrix = training_dataset.corr()\n",
    "# print(corr_matrix)\n",
    "\n",
    "training_dataset.info()\n",
    "# print(training_dataset[\"SugarSatisfied\"].value_counts())\n",
    "# print(training_dataset[\"FiberSatisfied\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # select randomly 50% of the traning dataset\n",
    "# # training_dataset = training_dataset.sample(frac=0.1, random_state=seed).reset_index(drop=True)\n",
    "# # check calories distribution\n",
    "# selected_data = training_dataset[(training_dataset['Like'] == 1) & (training_dataset['LowSugar'] == 1)]['CarbohydrateContent']\n",
    "# selected_data = selected_data[selected_data < 1600]\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(selected_data.index, selected_data)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Calories\")\n",
    "# plt.title(\"Distribution of Calories when Like=1 and HighCalories=1\")\n",
    "# plt.show()\n",
    "\n",
    "# selected_data = training_dataset[(training_dataset['Like'] == 1) & (training_dataset['HighCalories'] == 0)]['Calories']\n",
    "# selected_data = selected_data[selected_data < 1600]\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(selected_data.index, selected_data)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Calories\")\n",
    "# plt.title(\"Distribution of Calories when Like=1 and HighCalories=0\")\n",
    "# plt.show()\n",
    "\n",
    "# selected_data = training_dataset[(training_dataset['Like'] == 0) & (training_dataset['HighCalories'] == 1)]['Calories']\n",
    "# selected_data = selected_data[selected_data < 1000]\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(selected_data.index, selected_data)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Calories\")\n",
    "# plt.title(\"Distribution of Calories when Like=0 and HighCalories=1\")\n",
    "# plt.show()\n",
    "\n",
    "# selected_data = training_dataset[(training_dataset['Like'] == 0) & (training_dataset['HighCalories'] == 0)]['Calories']\n",
    "# selected_data = selected_data[selected_data < 1000]\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(selected_data.index, selected_data)\n",
    "# plt.xlabel(\"Index\")\n",
    "# plt.ylabel(\"Calories\")\n",
    "# plt.title(\"Distribution of Calories when Like=0 and HighCalories=0\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import sys\n",
    "import os\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_dataset.drop(columns=\"Like\"), training_dataset[\"Like\"], test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter: {'model': LogisticRegression(), 'model__C': 10, 'model__class_weight': 'balanced', 'pca__n_components': 11} (CV score=0.720)\n"
     ]
    }
   ],
   "source": [
    "# data scaling\n",
    "transform_scaler = StandardScaler()\n",
    "\n",
    "# dimensionality reduction\n",
    "transform_pca = PCA()\n",
    "model_logistic_regression = LogisticRegression()\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "model_ada_boosting = AdaBoostClassifier()\n",
    "\n",
    "# # train the models\n",
    "pipeline = Pipeline(steps=[(\"scaler\", transform_scaler), \n",
    "                           (\"pca\", transform_pca),\n",
    "                           (\"model\", None)])\n",
    "\n",
    "parameter_grid_preprocessing = {\n",
    "  \"pca__n_components\" : [7,8,9,10,11]\n",
    "}\n",
    "\n",
    "parameter_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],\n",
    "  \"model__class_weight\" : [\"balanced\"]\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [100],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [60, 70, 80, 90],\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [90, 100],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [60, 70, 80],\n",
    "  \"model__class_weight\" : [\"balanced\"]\n",
    "}\n",
    "\n",
    "parameter_ada_boosting = {\n",
    "  \"model\" : [model_ada_boosting],\n",
    "  \"model__n_estimators\" : [50, 100, 150],  # number of max trees in the forest\n",
    "  \"model__learning_rate\" : [0.8],\n",
    "}\n",
    "\n",
    "\n",
    "meta_parameter_grid = [parameter_logistic_regression]\n",
    "\n",
    "# meta_parameter_grid = [parameter_ada_boosting]\n",
    "\n",
    "meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=4, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.7172658510616257\n"
     ]
    }
   ],
   "source": [
    "balance_acc = balanced_accuracy_score(y_test, search.best_estimator_.predict(X_test))\n",
    "print(\"balanced accuracy:\", balance_acc)\n",
    "\n",
    "# best parameter: {'model': LogisticRegression(), 'model__C': 0.1, 'model__class_weight': 'balanced', 'pca__n_components': 7} (CV score=0.709)\n",
    "# dividing line for calories: 800\n",
    "# dividing line for fat: 10\n",
    "# dividing line for sugar: 5\n",
    "# dividing line for fiber: 8\n",
    "\n",
    "# version 2 without age\n",
    "# best parameter: {'model': LogisticRegression(), 'model__C': 0.05, 'model__class_weight': 'balanced', 'pca__n_components': 6} (CV score=0.642)\n",
    "\n",
    "# version 3 with age classify\n",
    "# best parameter: {'model': RandomForestClassifier(), 'model__class_weight': 'balanced', 'model__max_depth': 70, 'model__n_estimators': 90, 'pca__n_components': 6} (CV score=0.698)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.7172658510616257\n",
      "true      0     1\n",
      "pred             \n",
      "0     15014   895\n",
      "1      6097  2340\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of model on test set\n",
    "print(\"Score on test set:\", search.score(X_test, y_test.values.ravel()))\n",
    "\n",
    "# contingency table\n",
    "ct = pd.crosstab(search.best_estimator_.predict(X_test), y_test.values.ravel(),\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = search.best_estimator_.predict(submission_dataset)\n",
    "predictions = pd.DataFrame(predictions, columns=[\"prediction\"])\n",
    "predictions.index = predictions.index + 1\n",
    "predictions.to_csv(\"submission.csv\", index=True, index_label=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
